# Base Flux2 Klein 4B config â€” model, dataset, loss, optimizer. Rarely changes.
# Experiment configs use extends: base.yaml and override train, output, validation.

dataset:
  class_name: nexus.data.precomputed_sstk_dataset:PrecomputedSSTKDataset
  kwargs:
    local: null
    resolution: 512
    latent_channels: 32
    text_embed_hidden: 7680  # 2560*3 for Flux.2 Klein
    shuffle: true

collate:
  class_name: nexus.data.precomputed_sstk_dataset:collate_precomputed

# Pipeline: load everything (vae, scheduler, text_encoder, etc.) from here via from_pretrained.
# DiT is loaded separately from model.dit.
pipeline:
  class_name: diffusers:Flux2KleinPipeline
  pretrained_model_name_or_path: black-forest-labs/FLUX.2-klein-base-4B

model:
  dit:
    class_name: diffusers:Flux2Transformer2DModel
    subfolder: transformer
  transformer_wrapper:
    class_name: nexus.models.transformer_wrapper:TransformerWrapper
    component_name: transformer

train_mode: lora

lora:
  rank: 4
  alpha: 4
  dropout: 0.0
  target_modules: ["to_k", "to_q", "to_v", "to_out.0"]

train:
  batch_size: 4
  max_steps: null   # required or use 1 epoch; CLI: --max_train_steps
  gradient_accumulation_steps: 1
  gradient_checkpointing: false
  learning_rate: 1.0e-4
  guidance_scale: 3.5
  scale_lr: false
  lr_scheduler: constant
  lr_warmup_steps: 500
  lr_num_cycles: 1
  lr_power: 1.0
  dataloader_num_workers: 0

loss:
  class_name: nexus.train.losses:FlowMatchingLoss
  weighting_scheme: none
  logit_mean: 0.0
  logit_std: 1.0
  mode_scale: 1.29
  kwargs:
    base: mse

optimizer:
  class_name: torch.optim:AdamW
  kwargs:
    betas: [0.9, 0.999]
    weight_decay: 0.0001
    eps: 1.0e-8
max_grad_norm: 1.0

validation:
  steps: 500
  prompt: null
  num_images: 4
  seed: 42

output_dir: flux2-precomputed-lora
logging_dir: logs
checkpointing_steps: 500
checkpoints_total_limit: null
resume_from_checkpoint: null
seed: null

allow_tf32: false
report_to: mlflow
mixed_precision: null

mlflow:
  tracking_uri: null
  experiment_name: nexus-flux2
