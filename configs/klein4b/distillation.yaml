# Distillation training: teacher created inside loss, student learns from flow + teacher.
# Usage: accelerate launch -m nexus.train.main --config configs/klein4b/distillation.yaml

extends: base.yaml

loss:
  class_name: nexus.train.losses:DistillationLoss
  kwargs:
    base: mse
    pretrained_model_name_or_path: black-forest-labs/FLUX.2-klein-base-4B
    flow_weight: 0.5
    distillation_weight: 0.5

train:
  batch_size: 4
  max_steps: 1000
  learning_rate: 1.0e-4

output_dir: flux2-precomputed-distillation
checkpointing_steps: 500
