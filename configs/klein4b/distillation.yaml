# Distillation: flow + teacher. latent_channels, text_embed_hidden from base.
extends: base.yaml

dataset:
  class_name: nexus.data.precomputed_sstk_dataset:PrecomputedSSTKDataset
  kwargs:
    local: null
    resolution: 512
    shuffle: true

collate:
  class_name: nexus.data.precomputed_sstk_dataset:collate_precomputed

model:
  dit:
    class_name: diffusers:Flux2Transformer2DModel
    subfolder: transformer

train_mode: lora

lora:
  rank: 4
  alpha: 4
  dropout: 0.0
  target_modules: ["to_k", "to_q", "to_v", "to_out.0"]

train:
  batch_size: 4
  max_steps: 1000
  gradient_accumulation_steps: 1
  gradient_checkpointing: false
  learning_rate: 1.0e-4
  guidance_scale: 3.5
  scale_lr: false
  lr_scheduler: constant
  lr_warmup_steps: 500
  lr_num_cycles: 1
  lr_power: 1.0
  dataloader_num_workers: 0

loss:
  class_name: nexus.train.losses:DistillationLoss
  weighting_scheme: none
  logit_mean: 0.0
  logit_std: 1.0
  mode_scale: 1.29
  kwargs:
    base: mse
    pretrained_model_name_or_path: black-forest-labs/FLUX.2-klein-base-4B
    flow_weight: 0.5
    distillation_weight: 0.5

optimizer:
  class_name: torch.optim:AdamW
  kwargs:
    betas: [0.9, 0.999]
    weight_decay: 0.0001
    eps: 1.0e-8
max_grad_norm: 1.0

validation:
  steps: 500
  prompt: null
  num_images: 4
  seed: 42

output_dir: flux2-precomputed-distillation
logging_dir: logs
checkpointing_steps: 500
resume_from_checkpoint: null
seed: null

allow_tf32: false
report_to: mlflow
mixed_precision: null

mlflow:
  tracking_uri: null
  experiment_name: nexus-flux2
